{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing numpy\n",
    "import numpy as np\n",
    "\n",
    "# Importing pandas\n",
    "import pandas as pd\n",
    "\n",
    "# Importing PCA\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Importing MDS\n",
    "from sklearn.manifold import MDS\n",
    "\n",
    "# Importing T-SNE\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "# Importing train test split function\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Importing Seaborn\n",
    "import seaborn as sns\n",
    "\n",
    "# Importing matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Importing K-Means Clustering\n",
    "from sklearn.cluster import KMeans, DBSCAN\n",
    "\n",
    "# Importing Logistic Regression Cross-Validation function\n",
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "\n",
    "# Importing Warnings\n",
    "import warnings\n",
    "\n",
    "# Importing pickle\n",
    "import pickle\n",
    "\n",
    "# suppress FutureWarning\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the data\n",
    "X=np.load(\"X2.npy\")\n",
    "\n",
    "# Printing the shape of the resulting datasets\n",
    "print(f\"X shape: Number of cells (rows) {X.shape[0]} | Number of genes (columns) {X.shape[1]}\")\n",
    "\n",
    "# Calculating max X value for the first column\n",
    "print(f\"Max X for the first column: {np.max(X[:,0])}\")\n",
    "\n",
    "# Transforming the X data to log2\n",
    "X_log = np.log2(X+1)\n",
    "\n",
    "# Calculating max Xlog value for the first column\n",
    "print(f\"Max Xlog for the first column: {np.max(X_log[:,0])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clustering cells\n",
    "kmeans_X_log = KMeans(n_clusters=3, n_init=100).fit(X_log)\n",
    "\n",
    "# Calculating PCA for the X_log matrix\n",
    "pca_X_log = PCA(n_components=0.85)\n",
    "pca_X_log.fit(X_log.T);\n",
    "\n",
    "# Print summary of the PCA calculation result\n",
    "print(f\"Explained variance: {round(np.sum(pca_X_log.explained_variance_ratio_),3)}\")\n",
    "print(f\"Number of components ('Genes' components): {pca_X_log.n_components_}\")\n",
    "\n",
    "# Creating a PCA embedding with the X_log data\n",
    "pca_embedding_X_log = pca_X_log.components_.T\n",
    "\n",
    "# Printing the shape of the resulting datasets\n",
    "print(f\"X_log PCA shape: Number of cells (rows) {pca_embedding_X_log.shape[0]} | Number of PC genes (columns) {pca_embedding_X_log.shape[1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the size of the plot\n",
    "plt.figure(figsize=(8, 6))\n",
    "\n",
    "# Plotting clusters in the T-SNE embedding\n",
    "sns.scatterplot(x=pca_embedding_X_log[:,0], y=pca_embedding_X_log[:,1], hue=kmeans_X_log.labels_)\n",
    "\n",
    "# add custom title, x-label, and y-label\n",
    "plt.title(\"Brain cells types explained by the two first principal components (PC) for genes using PCA embedding\")\n",
    "plt.xlabel(\"PC1 for Genes\")\n",
    "plt.ylabel(\"PC2 for Genes\")\n",
    "\n",
    "# display the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a MDS embedding\n",
    "mds_embedding_X_log = MDS(n_components=2).fit_transform(X_log)\n",
    "\n",
    "# set the size of the plot\n",
    "plt.figure(figsize=(8, 6))\n",
    "\n",
    "# Plotting clusters in the T-SNE embedding\n",
    "sns.scatterplot(x=mds_embedding_X_log[:,0], y=mds_embedding_X_log[:,1], hue=kmeans_X_log.labels_)\n",
    "\n",
    "# add custom title, x-label, and y-label\n",
    "plt.title(\"Brain cells types explained by the two first principal components (PC) for genes using MDS embedding\")\n",
    "plt.xlabel(\"PC1 for Genes\")\n",
    "plt.ylabel(\"PC2 for Genes\")\n",
    "\n",
    "# display the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a T-SNE embedding with the PCA embedding data\n",
    "tsne_embedding_X_log = TSNE(n_components=2, perplexity=55).fit_transform(X_log)\n",
    "\n",
    "# set the size of the plot\n",
    "plt.figure(figsize=(8, 6))\n",
    "\n",
    "# Plotting clusters in the T-SNE embedding\n",
    "sns.scatterplot(x=tsne_embedding_X_log[:,0], y=tsne_embedding_X_log[:,1], hue=kmeans_X_log.labels_+1)\n",
    "\n",
    "# add custom title, x-label, and y-label\n",
    "plt.title(f\"Brain cells sub-types explained by genes principal components (PC) using t-SNE embedding (Perplexity: {55})\")\n",
    "plt.xlabel(\"Genes PC1\")\n",
    "plt.ylabel(\"Genes PC2\")\n",
    "\n",
    "# display the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating cluster using DBSCAN\n",
    "dbscan_clusters = DBSCAN(eps=1.51, min_samples=10).fit(tsne_embedding_X_log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_df = (\n",
    "    pd.DataFrame(tsne_embedding_X_log, columns=['PC1', 'PC2'], index=dbscan_clusters.labels_+1)\n",
    "    .reset_index()\n",
    "    .rename(columns={'index':'cell_sub_type'})\n",
    "    .assign(cell_type = lambda x:np.where(x.cell_sub_type.isin([0,1,2,3]), 1, np.where(x.cell_sub_type.isin([8,9,10,11]), 2, 3)))\n",
    "    [['cell_type', 'cell_sub_type', 'PC1', 'PC2']]\n",
    ")\n",
    "\n",
    "cluster_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    cluster_df\n",
    "    .assign(PC1_min = lambda x:x.PC1)\n",
    "    .assign(PC1_max = lambda x:x.PC1)\n",
    "    .assign(PC1_mean = lambda x:x.PC1)\n",
    "    .assign(PC2_min = lambda x:x.PC2)\n",
    "    .assign(PC2_max = lambda x:x.PC2)\n",
    "    .assign(PC2_mean = lambda x:x.PC2)\n",
    "    .groupby(['cell_type'])\n",
    "    .agg({\n",
    "        'PC1_min':np.min,\n",
    "        'PC1_max':np.max,\n",
    "        'PC1_mean':np.mean,\n",
    "        'PC2_min':np.min,\n",
    "        'PC2_max':np.max,\n",
    "        'PC2_mean':np.mean\n",
    "    })\n",
    "    .reset_index()\n",
    "    .assign(PC1_Domain = lambda x: [f\"[{round(i,2)}, {round(j,2)}]\" for i, j in zip(x.PC1_min, x.PC1_max)])\n",
    "    .assign(PC2_Domain = lambda x: [f\"[{round(i,2)}, {round(j,2)}]\" for i, j in zip(x.PC2_min, x.PC2_max)])\n",
    "    .assign(Center = lambda x: [f\"[{round(i,2)}, {round(j,2)}]\" for i, j in zip(x.PC1_mean, x.PC2_mean)])\n",
    "    [['cell_type', 'PC1_Domain', 'PC2_Domain', 'Center']]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a figure with 3 subplots in a row\n",
    "fig, axs = plt.subplots(1, 3, figsize=(16, 4))\n",
    "\n",
    "# create a scatter plot in each subplot\n",
    "sns.scatterplot(data=cluster_df[cluster_df.cluster.isin([0,1,2,3])], x=\"PC1\", y=\"PC2\", hue=\"cluster\", ax=axs[0], palette=sns.color_palette(\"hls\", 4))\n",
    "sns.scatterplot(data=cluster_df[cluster_df.cluster.isin([8,9,10,11])], x=\"PC1\", y=\"PC2\", hue=\"cluster\", ax=axs[1], palette=sns.color_palette(\"hls\", 4))\n",
    "sns.scatterplot(data=cluster_df[cluster_df.cluster.isin([4,5,6,7])], x=\"PC1\", y=\"PC2\", hue=\"cluster\", ax=axs[2], palette=sns.color_palette(\"hls\", 4))\n",
    "\n",
    "# add custom titles to each subplot\n",
    "axs[0].set_title(\"Cell type I\")\n",
    "axs[1].set_title(\"Cell type II\")\n",
    "axs[2].set_title(\"Cell type III\")\n",
    "\n",
    "# add custom x-labels and y-labels to each subplot\n",
    "axs[0].set_xlabel(\"Genes PC1\")\n",
    "axs[0].set_ylabel(\"Genes PC2\")\n",
    "axs[1].set_xlabel(\"Genes PC1\")\n",
    "axs[1].set_ylabel(\"Genes PC2\")\n",
    "axs[2].set_xlabel(\"Genes PC1\")\n",
    "axs[2].set_ylabel(\"Genes PC2\")\n",
    "\n",
    "# adjust the space between subplots\n",
    "plt.subplots_adjust(wspace=0.3)\n",
    "\n",
    "# display the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the size of the plot\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "sns.scatterplot(data=cluster_df, x=\"PC1\", y=\"PC2\", hue=\"cluster\", palette=sns.color_palette(\"hls\", 12))\n",
    "\n",
    "# add custom title, x-label, and y-label\n",
    "plt.title(f\"Brain cells sub-types explained by genes principal components (PC) using t-SNE embedding (Perplexity: {55}) - Clusters created using DBSCAN\")\n",
    "plt.xlabel(\"Genes PC1\")\n",
    "plt.ylabel(\"Genes PC2\")\n",
    "\n",
    "# display the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    cluster_df\n",
    "    .assign(cell_sub_type = lambda x: np.where(x.cell_sub_type == 0, 1, x.cell_sub_type))\n",
    "    .assign(PC1_min = lambda x:x.PC1)\n",
    "    .assign(PC1_max = lambda x:x.PC1)\n",
    "    .assign(PC1_mean = lambda x:x.PC1)\n",
    "    .assign(PC2_min = lambda x:x.PC2)\n",
    "    .assign(PC2_max = lambda x:x.PC2)\n",
    "    .assign(PC2_mean = lambda x:x.PC2)\n",
    "    .groupby(['cell_type', 'cell_sub_type'])\n",
    "    .agg({\n",
    "        'PC1_min':np.min,\n",
    "        'PC1_max':np.max,\n",
    "        'PC1_mean':np.mean,\n",
    "        'PC2_min':np.min,\n",
    "        'PC2_max':np.max,\n",
    "        'PC2_mean':np.mean\n",
    "    })\n",
    "    .reset_index()\n",
    "    .assign(PC1_Domain = lambda x: [f\"[{round(i,2)}, {round(j,2)}]\" for i, j in zip(x.PC1_min, x.PC1_max)])\n",
    "    .assign(PC2_Domain = lambda x: [f\"[{round(i,2)}, {round(j,2)}]\" for i, j in zip(x.PC2_min, x.PC2_max)])\n",
    "    .assign(Center = lambda x: [f\"[{round(i,2)}, {round(j,2)}]\" for i, j in zip(x.PC1_mean, x.PC2_mean)])\n",
    "    [['cell_type', 'cell_sub_type', 'PC1_Domain', 'PC2_Domain', 'Center']]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_df = (\n",
    "    cluster_df\n",
    "    .assign(cell_sub_type = lambda x: np.where(x.cell_sub_type == 0, 1, x.cell_sub_type))\n",
    "    [['cell_sub_type']]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#labels_df.to_csv(\"p2_cluster_labels_X_FT\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining X\n",
    "X = np.load(\"X2.npy\")\n",
    "\n",
    "# Transforming the X data to log2\n",
    "X_log = np.log2(X+1)\n",
    "\n",
    "# Defining y\n",
    "y = pd.read_csv(\"p2_cluster_labels_X_FT\").cell_sub_type.values\n",
    "\n",
    "# Splitting dataset into train, test and validation sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_log, y, test_size=0.30, stratify=y, random_state=42)\n",
    "\n",
    "print(f\"X_train size: {X_train.shape[0]} | y_train size: {y_train.shape[0]}\")\n",
    "print(f\"X_test  size: {X_test.shape[0]}  | y_test  size: {y_test.shape[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implementing cross-validation on logistic regression\n",
    "cv_clf = LogisticRegressionCV(\n",
    "    cv=5,                   # number of cross-validation folds\n",
    "    Cs=100,                 # number of regularization values to attempt\n",
    "    penalty='l1',           # penalty to implement: l1 - Lasso and l2 - Ridge\n",
    "    solver='liblinear',     # solver that allows l1 and l2 regularizations\n",
    "    tol=1e-5,               # tolerance\n",
    "    max_iter=100,           # max number of iterations\n",
    "    multi_class='ovr',      # in a multi-class context it will be performed one-versus-all\n",
    "    random_state=42         # random state\n",
    ").fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pickle.dump(cv_clf, open(\"cv_clf.pickle\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading model\n",
    "cv_clf = pickle.load(open(\"cv_clf.pickle\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# overall performance\n",
    "print(f\"Overall Accuracy: {round(np.mean(1*(cv_clf.predict(X_test) == y_test)), 3)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    pd.DataFrame(zip(y_test, 1*(cv_clf.predict(X_test) == y_test)), columns=['Cluster Label', 'True Positive (Predicted)'])\n",
    "    .assign(Total = lambda x:1)\n",
    "    .groupby(['Cluster Label'])\n",
    "    .sum()\n",
    "    .reset_index()\n",
    "    .assign(Accuracy = lambda x:x['True Positive (Predicted)']/x.Total)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save indexes for the best 100 features\n",
    "#with open('index_100.npy', 'wb') as f:\n",
    "#    np.save(f, index_100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the X p2 evaluation data and applying the log2(X+1) transformation\n",
    "X_train_p2_eval = np.log2(np.load(\"X_train.npy\")+1)\n",
    "X_test_p2_eval  = np.log2(np.load(\"X_test.npy\") +1)\n",
    "\n",
    "# Selecting the 100 best features\n",
    "index_best_100 = (-np.sum(np.abs(cv_clf.coef_), axis=0)).argsort()[:100]\n",
    "X_train_p2_eval_best_100 = X_train_p2_eval[:,index_best_100]\n",
    "X_test_p2_eval_best_100   = X_test_p2_eval[:,index_best_100]\n",
    "\n",
    "# Selecting 100 random features\n",
    "np.random.seed(42)\n",
    "index_random_100 = np.random.choice(a=range(0, X_train_p2_eval.shape[1]), size=100)\n",
    "X_train_p2_eval_random_100 = X_train_p2_eval[:,index_random_100]\n",
    "X_test_p2_eval_random_100  = X_test_p2_eval[:,index_random_100]\n",
    "\n",
    "# Selecting the 100 features with the highest variance\n",
    "index_high_variance_100 = (-np.std(X_train_p2_eval, axis=0)).argsort()[:100]\n",
    "X_train_p2_eval_high_variance_100 = X_train_p2_eval[:,index_high_variance_100]\n",
    "X_test_p2_eval_high_variance_100  = X_test_p2_eval[:,index_high_variance_100]\n",
    "\n",
    "# Loading the y p2 evaluation data\n",
    "y_train_p2_eval = np.load(\"y_train.npy\")\n",
    "y_test_p2_eval = np.load(\"y_test.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Re-evaluate the model implementing cross-validation on logistic regression using the reduced dataset X_small\n",
    "cv_clf_best_100 = LogisticRegressionCV(\n",
    "    cv=5,                   # number of cross-validation folds\n",
    "    Cs=100,                 # number of regularization values to attempt\n",
    "    penalty='l1',           # penalty to implement: l1 - Lasso and l2 - Ridge\n",
    "    solver='liblinear',     # solver that allows l1 and l2 regularizations\n",
    "    tol=1e-5,               # tolerance\n",
    "    max_iter=1000,          # max number of iterations\n",
    "    multi_class='ovr',      # in a multi-class context it will be performed one-versus-all\n",
    "    random_state=42         # random state\n",
    ").fit(X_train_p2_eval_best_100, y_train_p2_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(cv_clf_best_100, open(\"cv_clf_best_100.pickle\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Re-evaluate the model implementing cross-validation on logistic regression using the reduced dataset X_small\n",
    "cv_clf_random_100 = LogisticRegressionCV(\n",
    "    cv=5,                  # number of cross-validation folds\n",
    "    Cs=100,                 # number of regularization values to attempt\n",
    "    penalty='l1',           # penalty to implement: l1 - Lasso and l2 - Ridge\n",
    "    solver='liblinear',     # solver that allows l1 and l2 regularizations\n",
    "    tol=1e-3,               # tolerance\n",
    "    max_iter=100,          # max number of iterations\n",
    "    multi_class='ovr',      # in a multi-class context it will be performed one-versus-all\n",
    "    random_state=42         # random state\n",
    ").fit(X_train_p2_eval_random_100, y_train_p2_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(cv_clf_random_100, open(\"cv_clf_random_100.pickle\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Re-evaluate the model implementing cross-validation on logistic regression using the reduced dataset X_small\n",
    "cv_clf_high_variance_100 = LogisticRegressionCV(\n",
    "    cv=5,                   # number of cross-validation folds\n",
    "    Cs=100,                 # number of regularization values to attempt\n",
    "    penalty='l1',           # penalty to implement: l1 - Lasso and l2 - Ridge\n",
    "    solver='liblinear',     # solver that allows l1 and l2 regularizations\n",
    "    tol=1e-5,               # tolerance\n",
    "    max_iter=1000,          # max number of iterations\n",
    "    multi_class='ovr',      # in a multi-class context it will be performed one-versus-all\n",
    "    random_state=42         # random state\n",
    ").fit(X_train_p2_eval_high_variance_100, y_train_p2_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(cv_clf_high_variance_100, open(\"cv_clf_high_variance_100.pickle\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# overall performance\n",
    "print(f\"Best 100 features overall Accuracy: {round(np.mean(1*(cv_clf_best_100.predict(X_test_p2_eval_best_100) == y_test_p2_eval)), 3)}\")\n",
    "print(f\"Random 100 features overall Accuracy: {round(np.mean(1*(cv_clf_random_100.predict(X_test_p2_eval_random_100) == y_test_p2_eval)), 3)}\")\n",
    "print(f\"High Variance 100 features overall Accuracy: {round(np.mean(1*(cv_clf_high_variance_100.predict(X_test_p2_eval_high_variance_100) == y_test_p2_eval)), 3)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Best 100 features overall Accuracy: 0.889\n",
    "# Random 100 features overall Accuracy: 0.206\n",
    "# High Variance 100 features overall Accuracy: 0.892"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "var_df = (\n",
    "    pd.concat([\n",
    "        pd.DataFrame(zip(np.var(X_train_p2_eval_high_variance_100, axis=0), 100*['Highest Variance Features']), columns=['Variance', 'Features Set']),\n",
    "        pd.DataFrame(zip(np.var(X_train_p2_eval_best_100, axis=0), 100*['Best Features']), columns=['Variance', 'Features Set'])\n",
    "    ], axis=0)\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "var_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.color_palette(\"hls\", 4)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the size of the plot\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "sns.histplot(data=var_df, x=\"Variance\", hue=\"Features Set\", palette=sns.color_palette(\"hls\", 2))\n",
    "\n",
    "# add custom title, x-label, and y-label\n",
    "plt.title(f\"Variance Distribution | Highest Variance Features versus Best Features\")\n",
    "plt.xlabel(\"Variance\")\n",
    "plt.ylabel(\"Density\")\n",
    "\n",
    "# display the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.concat([\n",
    "    pd.DataFrame(np.var(X_train_p2_eval, axis=0), columns=['Variance (All Features)']).describe(),\n",
    "    pd.DataFrame(np.var(X_train_p2_eval_random_100, axis=0), columns=['variance (Random Features)']).describe(),\n",
    "    pd.DataFrame(np.var(X_train_p2_eval_best_100, axis=0), columns=['variance (Best Features)']).describe(),\n",
    "    pd.DataFrame(np.var(X_train_p2_eval_high_variance_100, axis=0), columns=['variance (High Variance Features)']).describe(),\n",
    "], axis=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# set the size of the plot\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "sns.histplot(x=np.var(X_train_p2_eval, axis=0))\n",
    "\n",
    "# add custom title, x-label, and y-label\n",
    "plt.title(f\"Variance Distribution | Highest Variance Features versus Best Features\")\n",
    "plt.xlabel(\"Variance\")\n",
    "plt.ylabel(\"Density\")\n",
    "\n",
    "# display the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a dataframe to hold cluster's labels\n",
    "cluster_label_df = pd.DataFrame(\n",
    "    y_test_p2_eval, \n",
    "    columns=['Cluster Label']\n",
    ")\n",
    "\n",
    "# Creating a dataframe to hold the results of the prediction using the model with th 100 best features\n",
    "prediction_best_100_df = pd.DataFrame(\n",
    "    1*(cv_clf_best_100.predict(X_test_p2_eval_best_100) == y_test_p2_eval), \n",
    "    columns=['True Positive (Predicted - 100 Best)']\n",
    ")\n",
    "\n",
    "# Creating a dataframe to hold the results of the prediction using the model with th 100 random features\n",
    "prediction_random_100_df = pd.DataFrame(\n",
    "    1*(cv_clf_random_100.predict(X_test_p2_eval_best_100) == y_test_p2_eval), \n",
    "    columns=['True Positive (Predicted - 100 Random)']\n",
    ")\n",
    "\n",
    "# Creating a dataframe to hold the results of the prediction using the model with th 100 highest variance features\n",
    "prediction_high_variance_100_df = pd.DataFrame(\n",
    "    1*(cv_clf_high_variance_100.predict(X_test_p2_eval_best_100) == y_test_p2_eval), \n",
    "    columns=['True Positive (Predicted - 100 Highest Variance)']\n",
    ")\n",
    "\n",
    "\n",
    "(\n",
    "    pd.concat([cluster_label_df, prediction_best_100_df, prediction_random_100_df, prediction_high_variance_100_df], axis=1)\n",
    "    .assign(Total = lambda x:1)\n",
    "    .groupby(['Cluster Label']).sum().reset_index()\n",
    "    #.assign(Accuracy = lambda x:x['True Positive (Predicted)']/x.Total)\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    pd.DataFrame(zip(y_test, 1*(cv_clf_best_100.predict(X_test) == y_test_p2_eval)), columns=['Cluster Label', 'True Positive (Predicted)'])\n",
    "    .assign(Total = lambda x:1)\n",
    "    .groupby(['Cluster Label'])\n",
    "    .sum()\n",
    "    .reset_index()\n",
    "    .assign(Accuracy = lambda x:x['True Positive (Predicted)']/x.Total)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(1*(preds == y_test_p2_eval))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    pd.DataFrame(zip(y_test_p2_eval, 1*(preds == y_test_p2_eval)), columns=['Cluster Label', 'True Positive (Predicted)'])\n",
    "    .assign(Total = lambda x:1)\n",
    "    .groupby(['Cluster Label'])\n",
    "    .sum()\n",
    "    .reset_index()\n",
    "    .assign(Accuracy = lambda x:x['True Positive (Predicted)']/x.Total)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the data\n",
    "X=np.load(\"X2.npy\")\n",
    "\n",
    "# Printing the shape of the resulting datasets\n",
    "print(f\"X shape: Number of cells (rows) {X.shape[0]} | Number of genes (columns) {X.shape[1]}\")\n",
    "\n",
    "# Calculating max X value for the first column\n",
    "print(f\"Max X for the first column: {np.max(X[:,0])}\")\n",
    "\n",
    "# Transforming the X data to log2\n",
    "X_log = np.log2(X+1)\n",
    "\n",
    "# Calculating max Xlog value for the first column\n",
    "print(f\"Max Xlog for the first column: {np.max(X_log[:,0])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clustering cells\n",
    "kmeans_X_log = KMeans(n_clusters=3, n_init=100).fit(X_log)\n",
    "\n",
    "# Calculating PCA for the X_log matrix\n",
    "pca_X_log10 = PCA(n_components=10).fit(X_log.T)\n",
    "pca_X_log50 = PCA(n_components=50).fit(X_log.T)\n",
    "pca_X_log100 = PCA(n_components=100).fit(X_log.T)\n",
    "pca_X_log250 = PCA(n_components=250).fit(X_log.T)\n",
    "pca_X_log500 = PCA(n_components=500).fit(X_log.T)\n",
    "\n",
    "# Creating a PCA embedding with the X_log data\n",
    "pca_embedding_X_log10 = pca_X_log10.components_.T\n",
    "pca_embedding_X_log50 = pca_X_log50.components_.T\n",
    "pca_embedding_X_log100 = pca_X_log100.components_.T\n",
    "pca_embedding_X_log250 = pca_X_log250.components_.T\n",
    "pca_embedding_X_log500 = pca_X_log500.components_.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a T-SNE embedding with the PCA embedding data\n",
    "tsne_embedding_X_log10 = TSNE(n_components=2, perplexity=40, learning_rate=100).fit_transform(pca_embedding_X_log10)\n",
    "tsne_embedding_X_log50 = TSNE(n_components=2, perplexity=40).fit_transform(pca_embedding_X_log50)\n",
    "tsne_embedding_X_log100 = TSNE(n_components=2, perplexity=40).fit_transform(pca_embedding_X_log100)\n",
    "tsne_embedding_X_log250 = TSNE(n_components=2, perplexity=40).fit_transform(pca_embedding_X_log250)\n",
    "tsne_embedding_X_log500 = TSNE(n_components=2, perplexity=40).fit_transform(pca_embedding_X_log500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a T-SNE embedding with the PCA embedding data\n",
    "tsne_embedding_X_log10 = TSNE(n_components=2, perplexity=40, learning_rate=50).fit_transform(pca_embedding_X_log500)\n",
    "tsne_embedding_X_log50 = TSNE(n_components=2, perplexity=40, learning_rate=100).fit_transform(pca_embedding_X_log500)\n",
    "tsne_embedding_X_log100 = TSNE(n_components=2, perplexity=40, learning_rate=200).fit_transform(pca_embedding_X_log500)\n",
    "tsne_embedding_X_log250 = TSNE(n_components=2, perplexity=40, learning_rate=500).fit_transform(pca_embedding_X_log500)\n",
    "tsne_embedding_X_log500 = TSNE(n_components=2, perplexity=40, learning_rate=800).fit_transform(pca_embedding_X_log500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a figure with 3 subplots in a row\n",
    "fig, axs = plt.subplots(1, 5, figsize=(20, 4))\n",
    "\n",
    "# create a scatter plot in each subplot\n",
    "sns.scatterplot(x=tsne_embedding_X_log10[:,0], y=tsne_embedding_X_log10[:,1], hue=kmeans_X_log.labels_, ax=axs[0])\n",
    "sns.scatterplot(x=tsne_embedding_X_log50[:,0], y=tsne_embedding_X_log50[:,1], hue=kmeans_X_log.labels_, ax=axs[1])\n",
    "sns.scatterplot(x=tsne_embedding_X_log100[:,0], y=tsne_embedding_X_log100[:,1], hue=kmeans_X_log.labels_, ax=axs[2])\n",
    "sns.scatterplot(x=tsne_embedding_X_log250[:,0], y=tsne_embedding_X_log250[:,1], hue=kmeans_X_log.labels_, ax=axs[3])\n",
    "sns.scatterplot(x=tsne_embedding_X_log500[:,0], y=tsne_embedding_X_log500[:,1], hue=kmeans_X_log.labels_, ax=axs[4])\n",
    "\n",
    "# add custom titles to each subplot\n",
    "axs[0].set_title(\"10 PC's\")\n",
    "axs[1].set_title(\"50 PC's\")\n",
    "axs[2].set_title(\"100 PC's\")\n",
    "axs[3].set_title(\"250 PC's\")\n",
    "axs[4].set_title(\"500 PC's\")\n",
    "\n",
    "# add custom x-labels and y-labels to each subplot\n",
    "axs[0].set_xlabel(\"Genes PC1\")\n",
    "axs[0].set_ylabel(\"Genes PC2\")\n",
    "axs[1].set_xlabel(\"Genes PC1\")\n",
    "axs[1].set_ylabel(\"Genes PC2\")\n",
    "axs[2].set_xlabel(\"Genes PC1\")\n",
    "axs[2].set_ylabel(\"Genes PC2\")\n",
    "axs[3].set_xlabel(\"Genes PC1\")\n",
    "axs[3].set_ylabel(\"Genes PC2\")\n",
    "axs[4].set_xlabel(\"Genes PC1\")\n",
    "axs[4].set_ylabel(\"Genes PC2\")\n",
    "\n",
    "#axs[0].set_xlim([-75,75])\n",
    "#axs[0].set_ylim([-75,75])\n",
    "#axs[1].set_xlim([-75,75])\n",
    "#axs[1].set_ylim([-75,75])\n",
    "#axs[2].set_xlim([-75,75])\n",
    "#axs[2].set_ylim([-75,75])\n",
    "#axs[3].set_xlim([-75,75])\n",
    "#axs[3].set_ylim([-75,75])\n",
    "#axs[4].set_xlim([-75,75])\n",
    "#axs[4].set_ylim([-75,75])\n",
    "\n",
    "# adjust the space between subplots\n",
    "plt.subplots_adjust(wspace=0.3)\n",
    "\n",
    "# display the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the size of the plot\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "sns.scatterplot(x=teste[:,0], y=teste[:,1], hue=kmeans_X_log.labels_)\n",
    "\n",
    "# add custom title, x-label, and y-label\n",
    "plt.title(f\"Variance Distribution | Highest Variance Features versus Best Features\")\n",
    "plt.xlabel(\"Variance\")\n",
    "plt.ylabel(\"Density\")\n",
    "\n",
    "# display the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating an empty dictionary\n",
    "tsne_dict = {}\n",
    "\n",
    "# Defining list of parameter values\n",
    "parameter_values = [1, 3, 5, 10, 25, 50, 100, 200]\n",
    "\n",
    "# Defining shape of the final image\n",
    "nrows = 2\n",
    "ncols = 4\n",
    "\n",
    "# create a figure with 3 subplots in a row\n",
    "fig, axs = plt.subplots(nrows, ncols, figsize=(4*ncols, 5*nrows))\n",
    "\n",
    "# adjust the space between subplots\n",
    "plt.subplots_adjust(wspace=0.3)\n",
    "\n",
    "# Initializing indexes\n",
    "r = 0\n",
    "c = 0\n",
    "\n",
    "for i, p in enumerate(parameter_values):\n",
    "\n",
    "    # Creating a T-SNE embedding with the chosen perplexity value\n",
    "    tsne_p = TSNE(n_components=2, perplexity=p, learning_rate=200, early_exaggeration=12).fit_transform(X_log)\n",
    "\n",
    "    # Storing embedding\n",
    "    tsne_dict[f\"perplexity_{p}\"] = tsne_p\n",
    "\n",
    "    # Creating a dataframe using the t-SNE embedding\n",
    "    tsne_df = pd.DataFrame(np.concatenate([tsne_p, kmeans_X_log.labels_.reshape(-1,1)], axis=1), columns=['PC1', 'PC2', 'Cluster'])\n",
    "\n",
    "    # Calculating center   \n",
    "    center = (\n",
    "        tsne_df.groupby(['Cluster']).mean().reset_index()\n",
    "        .drop(columns=['Cluster'])\n",
    "        .mean()\n",
    "    ).values\n",
    "\n",
    "    # Create a scatter plot in each subplot\n",
    "    sns.scatterplot(x=tsne_df.PC1 - center[0], y=tsne_df.PC2 - center[1], hue=tsne_df.Cluster, ax=axs[r,c])\n",
    "\n",
    "    # Add custom titles to each subplot\n",
    "    axs[r,c].set_title(f'Perplexity {p}')\n",
    "\n",
    "    # Add custom x-labels and y-labels to each subplot\n",
    "    axs[r,c].set_xlabel(\"Genes PC1\")\n",
    "    axs[r,c].set_ylabel(\"Genes PC2\")\n",
    "\n",
    "    axs[r,c].set_xlim([-70,70])\n",
    "    axs[r,c].set_ylim([-70,70])\n",
    "    \n",
    "    # updating column index\n",
    "    c += 1\n",
    "\n",
    "    # Adjusting subplot indexes\n",
    "    if(c == ncols):\n",
    "        c = 0\n",
    "        r += 1\n",
    "\n",
    "# display the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining list of parameter values\n",
    "parameter_values = [1, 3, 5, 10, 25, 50, 100, 200]\n",
    "\n",
    "# Defining shape of the final image\n",
    "nrows = 2\n",
    "ncols = 4\n",
    "\n",
    "# create a figure with 3 subplots in a row\n",
    "fig, axs = plt.subplots(nrows, ncols, figsize=(4*ncols, 4*nrows))\n",
    "\n",
    "# adjust the space between subplots\n",
    "plt.subplots_adjust(wspace=0.4, hspace=0.4)\n",
    "\n",
    "# Initializing indexes\n",
    "r = 0\n",
    "c = 0\n",
    "\n",
    "for i, p in enumerate(parameter_values):\n",
    "\n",
    "    # Creating a dataframe using the t-SNE embedding\n",
    "    tsne_df = pd.DataFrame(np.concatenate([tsne_dict[f\"perplexity_{p}\"], kmeans_X_log.labels_.reshape(-1,1)], axis=1), columns=['PC1', 'PC2', 'Cluster'])\n",
    "\n",
    "    # Calculating center   \n",
    "    center = (\n",
    "        tsne_df.groupby(['Cluster']).mean().reset_index()\n",
    "        .drop(columns=['Cluster'])\n",
    "        .mean()\n",
    "    ).values\n",
    "\n",
    "    # Create a scatter plot in each subplot\n",
    "    sns.scatterplot(x=tsne_df.PC1 - center[0], y=tsne_df.PC2 - center[1], hue=tsne_df.Cluster, ax=axs[r,c])\n",
    "\n",
    "    # Add custom titles to each subplot\n",
    "    axs[r,c].set_title(f'Perplexity {p}')\n",
    "\n",
    "    # Add custom x-labels and y-labels to each subplot\n",
    "    axs[r,c].set_xlabel(\"Genes PC1\")\n",
    "    axs[r,c].set_ylabel(\"Genes PC2\")\n",
    "\n",
    "    #axs[r,c].set_xlim([-70,70])\n",
    "    #axs[r,c].set_ylim([-70,70])\n",
    "    \n",
    "    # updating column index\n",
    "    c += 1\n",
    "\n",
    "    # Adjusting subplot indexes\n",
    "    if(c == ncols):\n",
    "        c = 0\n",
    "        r += 1\n",
    "\n",
    "# display the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating an empty dictionary\n",
    "tsne_dict = {}\n",
    "\n",
    "# Defining list of parameter values\n",
    "parameter_values = [0.01, 10, 100, 500, 1000, 2500, 5000, 10000]\n",
    "\n",
    "# Defining shape of the final image\n",
    "nrows = 2\n",
    "ncols = 4\n",
    "\n",
    "# create a figure with 3 subplots in a row\n",
    "fig, axs = plt.subplots(nrows, ncols, figsize=(4*ncols, 5*nrows))\n",
    "\n",
    "# adjust the space between subplots\n",
    "plt.subplots_adjust(wspace=0.3)\n",
    "\n",
    "# Initializing indexes\n",
    "r = 0\n",
    "c = 0\n",
    "\n",
    "for i, p in enumerate(parameter_values):\n",
    "\n",
    "    # Creating a T-SNE embedding with the chosen perplexity value\n",
    "    tsne_p = TSNE(n_components=2, perplexity=50, learning_rate=p, early_exaggeration=12).fit_transform(X_log)\n",
    "\n",
    "    # Storing embedding\n",
    "    tsne_dict[f\"learning_rate_{p}\"] = tsne_p\n",
    "\n",
    "    # Creating a dataframe using the t-SNE embedding\n",
    "    tsne_df = pd.DataFrame(np.concatenate([tsne_p, kmeans_X_log.labels_.reshape(-1,1)], axis=1), columns=['PC1', 'PC2', 'Cluster'])\n",
    "\n",
    "    # Calculating center   \n",
    "    center = (\n",
    "        tsne_df.groupby(['Cluster']).mean().reset_index()\n",
    "        .drop(columns=['Cluster'])\n",
    "        .mean()\n",
    "    ).values\n",
    "\n",
    "    # Create a scatter plot in each subplot\n",
    "    sns.scatterplot(x=tsne_df.PC1 - center[0], y=tsne_df.PC2 - center[1], hue=tsne_df.Cluster, ax=axs[r,c])\n",
    "\n",
    "    # Add custom titles to each subplot\n",
    "    axs[r,c].set_title(f'Learning Rate {p}')\n",
    "\n",
    "    # Add custom x-labels and y-labels to each subplot\n",
    "    axs[r,c].set_xlabel(\"Genes PC1\")\n",
    "    axs[r,c].set_ylabel(\"Genes PC2\")\n",
    "\n",
    "    axs[r,c].set_xlim([-40,40])\n",
    "    axs[r,c].set_ylim([-40,40])\n",
    "    \n",
    "    # updating column index\n",
    "    c += 1\n",
    "\n",
    "    # Adjusting subplot indexes\n",
    "    if(c == ncols):\n",
    "        c = 0\n",
    "        r += 1\n",
    "\n",
    "# display the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining list of parameter values\n",
    "parameter_values = [0.01, 10, 100, 500, 1000, 2500, 5000, 10000]\n",
    "\n",
    "# Defining shape of the final image\n",
    "nrows = 2\n",
    "ncols = 4\n",
    "\n",
    "# create a figure with 3 subplots in a row\n",
    "fig, axs = plt.subplots(nrows, ncols, figsize=(4*ncols, 4*nrows))\n",
    "\n",
    "# adjust the space between subplots\n",
    "plt.subplots_adjust(wspace=0.4, hspace=0.4)\n",
    "\n",
    "# Initializing indexes\n",
    "r = 0\n",
    "c = 0\n",
    "\n",
    "for i, p in enumerate(parameter_values):\n",
    "\n",
    "    # Creating a dataframe using the t-SNE embedding\n",
    "    tsne_df = pd.DataFrame(np.concatenate([tsne_dict[f\"learning_rate_{p}\"], kmeans_X_log.labels_.reshape(-1,1)], axis=1), columns=['PC1', 'PC2', 'Cluster'])\n",
    "\n",
    "    # Calculating center   \n",
    "    center = (\n",
    "        tsne_df.groupby(['Cluster']).mean().reset_index()\n",
    "        .drop(columns=['Cluster'])\n",
    "        .mean()\n",
    "    ).values\n",
    "\n",
    "    # Create a scatter plot in each subplot\n",
    "    sns.scatterplot(x=tsne_df.PC1 - center[0], y=tsne_df.PC2 - center[1], hue=tsne_df.Cluster, ax=axs[r,c])\n",
    "\n",
    "    # Add custom titles to each subplot\n",
    "    axs[r,c].set_title(f'Learning Rate {p}')\n",
    "\n",
    "    # Add custom x-labels and y-labels to each subplot\n",
    "    axs[r,c].set_xlabel(\"Genes PC1\")\n",
    "    axs[r,c].set_ylabel(\"Genes PC2\")\n",
    "\n",
    "    #axs[r,c].set_xlim([-70,70])\n",
    "    #axs[r,c].set_ylim([-70,70])\n",
    "    \n",
    "    # updating column index\n",
    "    c += 1\n",
    "\n",
    "    # Adjusting subplot indexes\n",
    "    if(c == ncols):\n",
    "        c = 0\n",
    "        r += 1\n",
    "\n",
    "# display the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining list of parameter values\n",
    "parameter_values = [0.01, 10, 100, 500, 1000, 2500, 5000, 10000]\n",
    "\n",
    "# Defining shape of the final image\n",
    "nrows = 2\n",
    "ncols = 4\n",
    "\n",
    "# create a figure with 3 subplots in a row\n",
    "fig, axs = plt.subplots(nrows, ncols, figsize=(4*ncols, 4*nrows))\n",
    "\n",
    "# adjust the space between subplots\n",
    "plt.subplots_adjust(wspace=0.4, hspace=0.4)\n",
    "\n",
    "# Initializing indexes\n",
    "r = 0\n",
    "c = 0\n",
    "\n",
    "for i, p in enumerate(parameter_values):\n",
    "\n",
    "    # Creating a dataframe using the t-SNE embedding\n",
    "    tsne_df = pd.DataFrame(np.concatenate([tsne_dict[f\"learning_rate_{p}\"], kmeans_X_log.labels_.reshape(-1,1)], axis=1), columns=['PC1', 'PC2', 'Cluster'])\n",
    "\n",
    "    # Calculating center   \n",
    "    center = (\n",
    "        tsne_df.groupby(['Cluster']).mean().reset_index()\n",
    "        .drop(columns=['Cluster'])\n",
    "        .mean()\n",
    "    ).values\n",
    "\n",
    "    # Create a scatter plot in each subplot\n",
    "    sns.scatterplot(x=tsne_df.PC1 - center[0], y=tsne_df.PC2 - center[1], hue=tsne_df.Cluster, ax=axs[r,c])\n",
    "\n",
    "    # Add custom titles to each subplot\n",
    "    axs[r,c].set_title(f'Learning Rate {p}')\n",
    "\n",
    "    # Add custom x-labels and y-labels to each subplot\n",
    "    axs[r,c].set_xlabel(\"Genes PC1\")\n",
    "    axs[r,c].set_ylabel(\"Genes PC2\")\n",
    "\n",
    "    axs[r,c].set_xlim([-50,60])\n",
    "    axs[r,c].set_ylim([-50,60])\n",
    "    \n",
    "    # updating column index\n",
    "    c += 1\n",
    "\n",
    "    # Adjusting subplot indexes\n",
    "    if(c == ncols):\n",
    "        c = 0\n",
    "        r += 1\n",
    "\n",
    "# display the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec1 = np.random.normal(loc=[10, 30], scale=[4,4], size=(500,2))\n",
    "vec2 = np.random.normal(loc=[20, -10], scale=[2,2], size=(500,2))\n",
    "vec3 = np.random.normal(loc=[-5, 5], scale=[5,5], size=(500,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.concatenate([vec1, vec2, vec3], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indexes = np.random.choice(list(range(X.shape[0])), size=1500, replace=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xshuffle = X[indexes,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "noise = np.random.uniform(low=8, high=10, size=(1500, 200))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xshuffle_noise = np.concatenate([Xshuffle, noise], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a T-SNE embedding with the chosen perplexity value\n",
    "tsne_X = TSNE(n_components=2, perplexity=50, learning_rate=500, early_exaggeration=12).fit_transform(Xshuffle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the size of the plot\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "sns.scatterplot(x=tsne_X[:,0], y=tsne_X[:,1])\n",
    "\n",
    "# add custom title, x-label, and y-label\n",
    "plt.title(f\"Variance Distribution | Highest Variance Features versus Best Features\")\n",
    "plt.xlabel(\"Variance\")\n",
    "plt.ylabel(\"Density\")\n",
    "\n",
    "# display the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a T-SNE embedding with the chosen perplexity value\n",
    "tsne_noise = TSNE(n_components=2, perplexity=50, learning_rate=500, early_exaggeration=12).fit_transform(Xshuffle_noise)\n",
    "\n",
    "# set the size of the plot\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "sns.scatterplot(x=tsne_noise[:,0], y=tsne_noise[:,1])\n",
    "\n",
    "# add custom title, x-label, and y-label\n",
    "plt.title(f\"Variance Distribution | Highest Variance Features versus Best Features\")\n",
    "plt.xlabel(\"Variance\")\n",
    "plt.ylabel(\"Density\")\n",
    "\n",
    "# display the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating PCA for the X_log matrix\n",
    "pca_Xnoise = PCA(n_components=50).fit(Xshuffle_noise.T);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a T-SNE embedding with the chosen perplexity value\n",
    "tsne_noise_pca = TSNE(n_components=2, perplexity=50, learning_rate=500, early_exaggeration=12).fit_transform(pca_Xnoise.components_.T)\n",
    "\n",
    "# set the size of the plot\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "sns.scatterplot(x=tsne_noise_pca[:,0], y=tsne_noise_pca[:,1])\n",
    "\n",
    "# add custom title, x-label, and y-label\n",
    "plt.title(f\"Variance Distribution | Highest Variance Features versus Best Features\")\n",
    "plt.xlabel(\"Variance\")\n",
    "plt.ylabel(\"Density\")\n",
    "\n",
    "# display the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing numpy\n",
    "import numpy as np\n",
    "\n",
    "# Importing pandas\n",
    "import pandas as pd\n",
    "\n",
    "# Importing PCA\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Importing MDS\n",
    "from sklearn.manifold import MDS\n",
    "\n",
    "# Importing T-SNE\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "# Importing train test split function\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Importing Seaborn\n",
    "import seaborn as sns\n",
    "\n",
    "# Importing matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Importing K-Means Clustering\n",
    "from sklearn.cluster import KMeans, DBSCAN\n",
    "\n",
    "# Importing Logistic Regression Cross-Validation function\n",
    "from sklearn.linear_model import LogisticRegressionCV, LogisticRegression\n",
    "\n",
    "# Importing Warnings\n",
    "import warnings\n",
    "\n",
    "# Importing pickle\n",
    "import pickle\n",
    "\n",
    "import time\n",
    "\n",
    "# suppress FutureWarning\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the X p2 evaluation data and applying the log2(X+1) transformation\n",
    "Xtr_eval = np.log2(np.load(\"X_train.npy\")+1)\n",
    "Xtt_eval = np.log2(np.load(\"X_test.npy\")+1)\n",
    "\n",
    "# Loading the y p2 evaluation data\n",
    "ytr_eval = np.load(\"y_train.npy\")\n",
    "ytt_eval = np.load(\"y_test.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "\n",
    "scaler.fit(Xtr_eval)\n",
    "\n",
    "XtrZ = scaler.transform(Xtr_eval)\n",
    "XttZ = scaler.transform(Xtt_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run Time 0: 44.53780150413513 seconds\n",
      "Run Time 1: 95.65845847129822 seconds\n",
      "Run Time 2: 189.26499557495117 seconds\n",
      "Run Time 3: 316.89289116859436 seconds\n",
      "Run Time 4: 469.7357437610626 seconds\n",
      "Run Time 5: 788.4033868312836 seconds\n",
      "Run Time 6: 1289.9108583927155 seconds\n",
      "Run Time 7: 2276.1246993541718 seconds\n",
      "Run Time 8: 2197.364904642105 seconds\n"
     ]
    }
   ],
   "source": [
    "cv_reg_dict = {}\n",
    "regs = [1e-6, 1e-5, 1e-4, 1e-3, 1e-2, 1e-1, 1e0, 1e1, 1e2, 1e3, 1e4, 1e5, 1e6]\n",
    "\n",
    "\n",
    "for i, r in enumerate(regs):\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Re-evaluate the model implementing cross-validation on logistic regression using the reduced dataset X_small\n",
    "    cv_clf = LogisticRegression(\n",
    "        C=r,\n",
    "        penalty='l2',\n",
    "        solver='liblinear', \n",
    "        tol=1e-4,\n",
    "        max_iter=1000,\n",
    "        multi_class='ovr',\n",
    "    ).fit(XtrZ, ytr_eval)\n",
    "\n",
    "    cv_reg_dict[i] = cv_clf\n",
    "\n",
    "    pickle.dump(cv_reg_dict, open(\"l2_models_dict_03.pickle\", \"wb\"))\n",
    "\n",
    "    print(f\"Run Time {i}: {time.time() - start_time} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run Time 0: 129.17098689079285 seconds\n",
      "Run Time 1: 130.89008784294128 seconds\n",
      "Run Time 2: 128.90697598457336 seconds\n",
      "Run Time 3: 159.2456865310669 seconds\n",
      "Run Time 4: 175.75416898727417 seconds\n",
      "Run Time 5: 226.1557378768921 seconds\n",
      "Run Time 6: 223.46430826187134 seconds\n",
      "Run Time 7: 224.54781818389893 seconds\n",
      "Run Time 8: 225.08366894721985 seconds\n",
      "Run Time 9: 232.03863525390625 seconds\n",
      "Run Time 10: 235.07268571853638 seconds\n",
      "Run Time 11: 248.48969554901123 seconds\n",
      "Run Time 12: 258.02161622047424 seconds\n",
      "Run Time 13: 262.0476715564728 seconds\n",
      "Run Time 14: 264.9710259437561 seconds\n",
      "Run Time 15: 283.5130796432495 seconds\n",
      "Run Time 16: 296.62924790382385 seconds\n"
     ]
    }
   ],
   "source": [
    "cv_reg_dict = {}\n",
    "regs = [1e-6, 1e-5, 1e-4, 1e-3, 1e-2, 1e-1, 1e0, 1e1, 1e2, 1e3, 1e4, 1e5, 1e6]\n",
    "\n",
    "\n",
    "for i, r in enumerate(regs):\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Re-evaluate the model implementing cross-validation on logistic regression using the reduced dataset X_small\n",
    "    cv_clf = LogisticRegression(\n",
    "        C=r,\n",
    "        penalty='l1',\n",
    "        solver='liblinear', \n",
    "        tol=1e-4,   \n",
    "        max_iter=1000,\n",
    "        multi_class='ovr',\n",
    "    ).fit(XtrZ, ytr_eval)\n",
    "\n",
    "    cv_reg_dict[i] = cv_clf\n",
    "\n",
    "    pickle.dump(cv_reg_dict, open(\"l1_models_dict_03.pickle\", \"wb\"))\n",
    "\n",
    "    print(f\"Run Time {i}: {time.time() - start_time} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading model\n",
    "l1_models = pickle.load(open(\"l1_models_dict_02.pickle\", \"rb\"))\n",
    "l2_models = pickle.load(open(\"l2_models_dict_02.pickle\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "l1_features_dict = {}\n",
    "l2_features_dict = {}\n",
    "\n",
    "for i, m in enumerate(l1_models):\n",
    "\n",
    "    # Calculating sum of coeficients and ordering it\n",
    "    sum_abs_coefs = np.sum(np.abs(l1_models[m].coef_), axis=0)\n",
    "    ordered_sum_abs_coefs = np.argsort(-sum_abs_coefs)[0:100]\n",
    "\n",
    "    l1_features_dict[i] = ordered_sum_abs_coefs\n",
    "\n",
    "\n",
    "for i, m in enumerate(l2_models):\n",
    "\n",
    "    # Calculating sum of coeficients and ordering it\n",
    "    sum_abs_coefs = np.sum(np.abs(l2_models[m].coef_), axis=0)\n",
    "    ordered_sum_abs_coefs = np.argsort(-sum_abs_coefs)[0:100]\n",
    "\n",
    "    l2_features_dict[i] = ordered_sum_abs_coefs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run Time 0: 0.9536426067352295 seconds\n",
      "Run Time 1: 0.7775146961212158 seconds\n",
      "Run Time 2: 0.8213367462158203 seconds\n",
      "Run Time 3: 1.078181505203247 seconds\n",
      "Run Time 4: 1.3847568035125732 seconds\n",
      "Run Time 5: 2.3351991176605225 seconds\n",
      "Run Time 6: 2.6817193031311035 seconds\n",
      "Run Time 7: 3.7748289108276367 seconds\n",
      "Run Time 8: 3.9816157817840576 seconds\n",
      "Run Time 9: 5.3312249183654785 seconds\n",
      "Run Time 10: 5.945649862289429 seconds\n",
      "Run Time 11: 6.834094762802124 seconds\n",
      "Run Time 12: 6.900559663772583 seconds\n",
      "Run Time 13: 6.84396505355835 seconds\n",
      "Run Time 14: 7.13137674331665 seconds\n",
      "Run Time 15: 7.450761079788208 seconds\n",
      "Run Time 16: 9.576491594314575 seconds\n"
     ]
    }
   ],
   "source": [
    "l2_models_100 = {}\n",
    "\n",
    "for i, m in enumerate(l2_features_dict):\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Re-evaluate the model implementing cross-validation on logistic regression using the reduced dataset X_small\n",
    "    cv_clf = LogisticRegression(\n",
    "        C=l2_models[i].Cs_[0],\n",
    "        penalty='l2',\n",
    "        solver='saga', \n",
    "        tol=1e-3,\n",
    "        max_iter=1000,\n",
    "        multi_class='ovr',\n",
    "        random_state=42\n",
    "    ).fit(XtrZ[:,l2_features_dict[i]], ytr_eval)\n",
    "\n",
    "    l2_models_100[i] = cv_clf\n",
    "\n",
    "    print(f\"Run Time {i}: {time.time() - start_time} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run Time 0: 0.052841901779174805 seconds\n",
      "Run Time 1: 0.050806522369384766 seconds\n",
      "Run Time 2: 0.08862495422363281 seconds\n",
      "Run Time 3: 0.44838476181030273 seconds\n",
      "Run Time 4: 2.115631341934204 seconds\n",
      "Run Time 5: 3.3491249084472656 seconds\n",
      "Run Time 6: 4.510924816131592 seconds\n",
      "Run Time 7: 8.584869861602783 seconds\n",
      "Run Time 8: 9.964682579040527 seconds\n",
      "Run Time 9: 11.677557229995728 seconds\n",
      "Run Time 10: 12.963160037994385 seconds\n",
      "Run Time 11: 13.097201585769653 seconds\n",
      "Run Time 12: 13.89504623413086 seconds\n",
      "Run Time 13: 13.943207502365112 seconds\n",
      "Run Time 14: 12.661410331726074 seconds\n",
      "Run Time 15: 13.012511491775513 seconds\n",
      "Run Time 16: 13.328543424606323 seconds\n"
     ]
    }
   ],
   "source": [
    "l1_models_100 = {}\n",
    "\n",
    "for i, m in enumerate(l1_features_dict):\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Re-evaluate the model implementing cross-validation on logistic regression using the reduced dataset X_small\n",
    "    cv_clf = LogisticRegression(\n",
    "        C=l1_models[i].Cs_[0],\n",
    "        penalty='l1',\n",
    "        solver='saga', \n",
    "        tol=1e-3,\n",
    "        max_iter=1000,\n",
    "        multi_class='ovr',\n",
    "        random_state=42\n",
    "    ).fit(XtrZ[:,l1_features_dict[i]], ytr_eval)\n",
    "\n",
    "    l1_models_100[i] = cv_clf\n",
    "\n",
    "    print(f\"Run Time {i}: {time.time() - start_time} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Regularization: 0.0001 | Score: 0.15433212996389892\n",
      "Regularization: 0.0005 | Score: 0.490072202166065\n",
      "Regularization: 0.001 | Score: 0.6245487364620939\n",
      "Regularization: 0.005 | Score: 0.8492779783393501\n",
      "Regularization: 0.01 | Score: 0.8889891696750902\n",
      "Regularization: 0.05 | Score: 0.9314079422382672\n",
      "Regularization: 0.1 | Score: 0.9305054151624549\n",
      "Regularization: 0.5 | Score: 0.9232851985559567\n",
      "Regularization: 1.0 | Score: 0.9205776173285198\n",
      "Regularization: 5.0 | Score: 0.9160649819494585\n",
      "Regularization: 10.0 | Score: 0.9133574007220217\n",
      "Regularization: 50.0 | Score: 0.9115523465703971\n",
      "Regularization: 100.0 | Score: 0.9106498194945848\n",
      "Regularization: 500.0 | Score: 0.9097472924187726\n",
      "Regularization: 1000.0 | Score: 0.9097472924187726\n",
      "Regularization: 5000.0 | Score: 0.9034296028880866\n",
      "Regularization: 10000.0 | Score: 0.9151624548736462\n"
     ]
    }
   ],
   "source": [
    "for i, m in enumerate(l2_models_100):\n",
    "\n",
    "    score = np.mean(1*(l2_models_100[i].predict(XttZ[:,l2_features_dict[i]]) == ytt_eval))\n",
    "\n",
    "    print(f\"Regularization: {l2_models_100[i].C} | Score: {score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Regularization: 0.0001 | Score: 0.15252707581227437\n",
      "Regularization: 0.0005 | Score: 0.15252707581227437\n",
      "Regularization: 0.001 | Score: 0.0631768953068592\n",
      "Regularization: 0.005 | Score: 0.2536101083032491\n",
      "Regularization: 0.01 | Score: 0.5577617328519856\n",
      "Regularization: 0.05 | Score: 0.8574007220216606\n",
      "Regularization: 0.1 | Score: 0.8628158844765343\n",
      "Regularization: 0.5 | Score: 0.8474729241877257\n",
      "Regularization: 1.0 | Score: 0.8366425992779783\n",
      "Regularization: 5.0 | Score: 0.8429602888086642\n",
      "Regularization: 10.0 | Score: 0.8185920577617328\n",
      "Regularization: 50.0 | Score: 0.7833935018050542\n",
      "Regularization: 100.0 | Score: 0.8438628158844765\n",
      "Regularization: 500.0 | Score: 0.8655234657039711\n",
      "Regularization: 1000.0 | Score: 0.8971119133574007\n",
      "Regularization: 5000.0 | Score: 0.8727436823104693\n",
      "Regularization: 10000.0 | Score: 0.8700361010830325\n"
     ]
    }
   ],
   "source": [
    "for i, m in enumerate(l1_models_100):\n",
    "\n",
    "    score = np.mean(1*(l1_models_100[i].predict(XttZ[:,l1_features_dict[i]]) == ytt_eval))\n",
    "\n",
    "    print(f\"Regularization: {l1_models_100[i].C} | Score: {score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.93355694,  1.37705335, -0.58626064, ..., -0.4508869 ,\n",
       "        -0.10034216, -0.27064737],\n",
       "       [-0.93355694,  1.21159526, -0.58626064, ..., -0.79143059,\n",
       "        -0.10034216, -0.27064737],\n",
       "       [-0.93355694,  1.50601528,  1.57850613, ...,  1.82502734,\n",
       "        -0.10034216, -0.27064737],\n",
       "       ...,\n",
       "       [-0.93355694, -1.43502578, -0.58626064, ...,  0.35315673,\n",
       "        -0.10034216, -0.27064737],\n",
       "       [-0.93355694, -1.43502578, -0.58626064, ..., -0.79143059,\n",
       "        -0.10034216, -0.27064737],\n",
       "       [-0.93355694, -1.1726448 , -0.58626064, ..., -0.25314279,\n",
       "        -0.10034216, -0.27064737]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, m in enumerate(l2_features_dict):\n",
    "\n",
    "    arr = np.var(Xtr_eval[:,l2_features_dict[i]], axis=0).reshape(-1,1)\n",
    "    if(i==0):\n",
    "        l2_var = arr\n",
    "    else:\n",
    "        l2_var = np.concatenate([l2_var, arr], axis=0)\n",
    "\n",
    "for i, m in enumerate(l1_features_dict):\n",
    "\n",
    "    arr = np.var(Xtr_eval[:,l1_features_dict[i]], axis=0).reshape(-1,1)\n",
    "    if(i==0):\n",
    "        l1_var = arr\n",
    "    else:\n",
    "        l1_var = np.concatenate([l1_var, arr], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = []\n",
    "for i in [1e-4, 5e-4, 1e-3, 5e-3, 1e-2, 5e-2, 1e-1, 5e-1, 1e0, 5e0, 1e1, 5e1, 1e2, 5e2, 1e3, 5e3, 1e4]:\n",
    "    labels += 100*[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "l2_var_df = (\n",
    "    pd.DataFrame(np.log(l2_var+1), columns=['variance'])\n",
    "    .assign(regularization = lambda x:labels)\n",
    "    .assign(regularization_type = lambda x: 'l2')\n",
    ")\n",
    "\n",
    "l1_var_df = (\n",
    "    pd.DataFrame(np.log(l1_var+1), columns=['variance'])\n",
    "    .assign(regularization = lambda x:labels)\n",
    "    .assign(regularization_type = lambda x: 'l1')\n",
    ")\n",
    "\n",
    "var_df = pd.concat([l2_var_df, l1_var_df], axis=0).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 864x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# set the size of the plot\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "sns.boxplot(x=var_df.regularization, y=var_df.variance, hue=var_df.regularization_type)\n",
    "\n",
    "# add custom title, x-label, and y-label\n",
    "#plt.title(f\"Variance Distribution | Highest Variance Features versus Best Features\")\n",
    "#plt.xlabel(\"Variance\")\n",
    "#plt.ylabel(\"Density\")\n",
    "\n",
    "# display the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
